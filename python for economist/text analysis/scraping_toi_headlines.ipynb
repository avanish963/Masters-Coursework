{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = r\"Put in your driver Path\"\n",
    "service = Service(driver_path)\n",
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#Creating a dataframe\n",
    "df=pd.DataFrame()\n",
    "\n",
    "#Starting from 1 January\n",
    "start_data=44927\n",
    "for i in range(start_data,start_data+365):\n",
    "    try:\n",
    "        # Step 1 Going to the page and scraping the link and the heading of the article\n",
    "        url = f\"https://timesofindia.indiatimes.com/2023/1/1/archivelist/year-2023,month-1,starttime-{i}.cms\"\n",
    "        driver.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        content = soup.find(\"td\", valign=\"top\")\n",
    "        links = content.find_all('a', href=True)\n",
    "        filtered_links = [link for link in links if link['href'].startswith('http')]\n",
    "\n",
    "        # Step 2: Create a list of tuples (link, heading)\n",
    "        data = [(link['href'], link.get_text(strip=True)) for link in filtered_links]\n",
    "        \n",
    "        # Step 3: Create a DataFrame\n",
    "        new_df = pd.DataFrame(data, columns=['Link', 'Heading'])\n",
    "        df=pd.concat([df,new_df],axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error occurred:\", str(e))\n",
    "        print(\"Skipping\", i)\n",
    "        continue\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"Put in your working directory\")\n",
    "newsarticles.to_csv(\"headlines_2023.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

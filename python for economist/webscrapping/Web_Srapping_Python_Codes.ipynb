{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver # to interact with the web browser\n",
    "from selenium.webdriver.common.by import By # to specify element-finding strategies\n",
    "from selenium.webdriver.chrome.options import Options # to configure browser options\n",
    "from selenium.webdriver.support.ui import WebDriverWait # to make Selenium wait until certain conditions (like element visibility) are met\n",
    "from selenium.webdriver.support import expected_conditions as EC # to specify condition while waiting\n",
    "from selenium.webdriver.common.keys import Keys # to mimic physical keyboard actions\n",
    "from bs4 import BeautifulSoup # to pull the content of HTML file into a format that can be parsed\n",
    "import pandas as pd # to handle and analyze data\n",
    "import time # to control waiting times during execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting up the webpage using chrome driver for further operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating webdriver with specified options and executable path object\n",
    "driver_path = r\"C:\\Users\\Dell\\Desktop\\chromedriver129\\chromedriver.exe\" # specify local path for chrome driver\n",
    "chrome_options = Options() # Configure Chrome options\n",
    "driver = webdriver.Chrome(executable_path = driver_path, options=chrome_options)\n",
    "\n",
    "# Loading the webpage\n",
    "url = \" https://enquiry.indianrail.gov.in/mntes/\"\n",
    "driver.get(url) # Opens the specified url\n",
    "\n",
    "# Find 'More Info' in the webpage and click (wait for atleast 30 seconds for the element to be located)\n",
    "link1 = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"sidebar\"]/form/ul/li[6]')))\n",
    "link1.click()\n",
    "time.sleep(5) # Wait for 5 second before continuing execution\n",
    "\n",
    "# Find 'Average Delay' in the webpage and click (wait for atleast 30 seconds for the element to be located)\n",
    "link2 = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"pageSubmenu2\"]/li[3]')))\n",
    "link2.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Looping through list containing train number to scrap raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv file containing train numbers\n",
    "Train_df = pd.read_csv('Train number.csv', dtype={'Train No': 'str'}) # dtype={'Train No': 'str'} ensures that train numbers starting with 0 do not lose the starting digit\n",
    "Train_list = Train_df['Train No'].to_list() # extract train numbers into a list\n",
    "\n",
    "# Loop through the train list, scrap data for each train number and save it as raw data\n",
    "for i in Train_list: # select one vlaue  from the train number list\n",
    "    try:\n",
    "        # find the input field for train numbers (wait for 30 seconds for the elemnt to be located), clear existing inputs and send new input for train number\n",
    "        input_field = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"trainNo\"]'))) # find the input field\n",
    "        input_field.send_keys(Keys.CONTROL + \"a\") # mimic keyboard action (ctrl + a) to select all content in the input field\n",
    "        input_field.send_keys(Keys.BACKSPACE) # mimic backspace key to clear input field\n",
    "        input_field.send_keys(f'{i}') # send new train number\n",
    "        time.sleep(2) # wait for 2 second before continuing execution\n",
    "\n",
    "        # find 'Go' in webpage and click (wait for 30 seconds for the elemnt to be located) \n",
    "        link3 = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"content\"]/form[2]/table/tbody/tr/td[3]/span/input')))\n",
    "        link3.click()\n",
    "        time.sleep(5) # wait for 5 second before continuing execution\n",
    "\n",
    "        # parses the HTML content from the current page loaded by Selenium's WebDriverpage using 'html.parser' and save as soup variable\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # selects all the table rows (<tr> elements) from the page\n",
    "        rows = soup.select(\"tr\")\n",
    "    \n",
    "\n",
    "        # create empty list to store data from tables\n",
    "        table_data = []\n",
    "\n",
    "        # loop through table rows\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\") # finds all the <td> elements representing data cells\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells] # extracts the text from each cell, strips out any extra spaces or line breaks and collects the text from all cells in the current row\n",
    "            table_data.append(row_data) # store it in the table_data list\n",
    "            \n",
    "        # create dataframe to store data for each train number, name the columns as 'sr_no', 'station','station_code','avg_arr_delay','avg_dep_delay'\n",
    "        data = pd.DataFrame(table_data, columns=['sr_no', 'station','station_code','avg_arr_delay','avg_dep_delay'])\n",
    "\n",
    "        # save data corresponding to each file in the folder \"Final Raw Data\" in current working directory in the format \"Train number.csv\"\n",
    "        data.to_csv(f'Final Raw Data/{i}.csv', index=False) \n",
    "\n",
    "    except ValueError: # handling 'ValueError' due to absence of data for some train numbers\n",
    "        print(f'No delay data available for Train No. {i}') # printing the train numbers that do not have delay data available\n",
    "    except TimeoutException:\n",
    "        print('Timed out: Continue to next code cell')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Cleaning raw data and scrapping for missing data\n",
    "\n",
    "Creating list of train numbers for which\n",
    "1. Data could not be scraped due to network issues\n",
    "2. No data is available\n",
    "\n",
    "Run the code below till network_error_scrap_again is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store train numbers with issues\n",
    "network_error_scrap_again = []  # empty list to store train numbers for which data could not be scraped\n",
    "no_delay_data = []  # empty list to store train numbers for which delay data is not available\n",
    "\n",
    "# First pass: loop through list of train numbers, clean the data, and save as CSV\n",
    "for i in Train_list:\n",
    "    try:\n",
    "        # Load data and add train number column\n",
    "        data = pd.read_csv(f'Final Raw Data/{i}.csv') # Load CSV file for each train where i is the train number\n",
    "        data['train_no'] = i # Add a new column train_no to store the train number in the dataframe for reference\n",
    "\n",
    "        # Extract and assign day of run and type of train\n",
    "        data['days'] = data['sr_no'][2].split(':')[1].strip()\n",
    "        data['type'] = data['station'][2].split(':')[1].strip()\n",
    "        \n",
    "        # Clean the data: drop irrelevant rows (first 4 and last), reset index, replace 'On Time' with '00:00', and fill NaN values with 0\n",
    "        data = data.drop(index=data.index[:4]).iloc[:-1].reset_index(drop=True)\n",
    "        data[['avg_arr_delay', 'avg_dep_delay']] = data[['avg_arr_delay', 'avg_dep_delay']].replace('On Time', '00:00')\n",
    "        data = data.fillna(0)\n",
    "        \n",
    "        # Save cleaned data\n",
    "        data.to_csv(f'Cleaned Data/{i}_cleaned.csv')\n",
    "\n",
    "    except FileNotFoundError:  # handle cases having no delay data and thus no raw data file\n",
    "        no_delay_data.append(i)\n",
    "        print(f'No delay data available for Train No. {i}')\n",
    "    except KeyError:  # handle cases having blank raw data csv file\n",
    "        network_error_scrap_again.append(i)\n",
    "        print(f'Data not scrapped for Train No. {i}')\n",
    "\n",
    "# Retry scraping for network errors until 'network_error_scrap_again' list is empty\n",
    "while network_error_scrap_again:\n",
    "    for i in network_error_scrap_again:\n",
    "        try:\n",
    "            # Wait for the input field to be present and interact with it\n",
    "            input_field = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"trainNo\"]')))\n",
    "            input_field.send_keys(Keys.CONTROL + \"a\")\n",
    "            input_field.send_keys(Keys.BACKSPACE)\n",
    "            input_field.send_keys(f'{i}')\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Click the button to submit the form\n",
    "            link3 = WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"content\"]/form[2]/table/tbody/tr/td[3]/span/input')))\n",
    "            link3.click()\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            rows = soup.select(\"tr\")\n",
    "            table_data = []\n",
    "\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "                table_data.append(row_data)\n",
    "\n",
    "            # Create a DataFrame and save it to CSV\n",
    "            data = pd.DataFrame(table_data, columns=[\"sr_no\", \"station\", 'station_code', 'avg_arr_delay', 'avg_dep_delay'])\n",
    "            data.to_csv(f'Final Raw Data/{i}.csv', index=False)\n",
    "\n",
    "            # Remove the processed train number from the list if successful\n",
    "            network_error_scrap_again.remove(i)\n",
    "            print(f'Successfully processed Train No. {i}')\n",
    "\n",
    "        except ValueError:\n",
    "            print(f'No delay data available for Train No. {i}')\n",
    "            network_error_scrap_again.remove(i)  # Remove from the list even if there's an error\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred while processing Train No. {i}: {e}')\n",
    "\n",
    "print(\"Train numbers not scrapped due to network error:\", network_error_scrap_again)\n",
    "print(\"No delay data train numbers:\", no_delay_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Preparing data for generating recommendation chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove train numbers with missing delay data\n",
    "Train_list = list(set(Train_list) - set(no_delay_data))\n",
    "\n",
    "# Load and merge CSV files\n",
    "dataframes = [pd.read_csv(f'Cleaned Data/{i}_cleaned.csv', dtype={'train_no': 'str'}) for i in Train_list]\n",
    "merged_df = pd.concat(dataframes, ignore_index=True).iloc[:, 2:]\n",
    "\n",
    "# Convert delay time from HH:MM to minutes\n",
    "def convert_time(time_str):\n",
    "    try:\n",
    "        hr, min = map(int, time_str.split(':')) # splits time_str wherever a colon : appears, which separates hours and minutes in a time string, store in a list and convert each element in list into integers\n",
    "        return hr * 60 + min\n",
    "    except (AttributeError, IndexError, ValueError):\n",
    "        return None\n",
    "\n",
    "data = merged_df.fillna(0)\n",
    "data['avg_arr_delay'] = data['avg_arr_delay'].apply(convert_time)\n",
    "data = data.fillna(0)\n",
    "data.drop(data.columns[[1, 3]], axis=1, inplace=True)\n",
    "\n",
    "# Save processed data to CSV\n",
    "data.to_csv('data_for_input_command.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generating recommendations based on the choice of destination and day of journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get destination input, load data, and filter by common characters in the station name\n",
    "destination = input('Enter destination (city name or station name (when city and station names are different: )').upper()\n",
    "data = pd.read_csv('data_for_input_command.csv', dtype={'train_no': 'str'})\n",
    "\n",
    "# Use str.contains to find stations with common characters ('na=False' - treats NaN values as if they don't match)\n",
    "output = data[data['station'].str.contains(destination, case=False, na=False)].sort_values(by='avg_arr_delay')[['train_no', 'station', 'avg_arr_delay', 'days', 'type']]\n",
    "\n",
    "# Ask for day input\n",
    "day_input = input('Enter the day of the week: ').strip().lower()\n",
    "\n",
    "# Use only the first three characters for matching\n",
    "day_input_short = day_input[:3]  # Get the first three characters\n",
    "\n",
    "# Match input day with the days of run (including \"daily\" as a match for any day)\n",
    "if day_input != \"daily\":\n",
    "    output = output[output['days'].str.contains(day_input_short, case=False, na=False) | output['days'].str.contains(\"daily\", case=False)]\n",
    "\n",
    "# Rename columns and reset index to start from 1, excluding the 'days' column\n",
    "output = output[['train_no', 'station', 'avg_arr_delay', 'type']].rename(columns={\n",
    "    'train_no': 'Train Number',\n",
    "    'station': 'Station',\n",
    "    'avg_arr_delay': 'Delay Time (in Minutes)', \n",
    "    'type': 'Type'\n",
    "})\n",
    "output.reset_index(drop=True, inplace=True)\n",
    "output.index += 1  # Start index from 1\n",
    "\n",
    "print(f\"Recommendation chart for Journey to {destination} on {day_input.capitalize()}\")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_economists",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
